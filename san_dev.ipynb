{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NepaliSoundDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        annotations_file_path,\n",
    "        audio_dir,\n",
    "    ):\n",
    "        self.annotaions = pd.read_csv(annotations_file_path, sep=\"\\t\")\n",
    "        self.audio_dir = audio_dir        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotaions)\n",
    "\n",
    "    def _get_speaker_id(self, index):\n",
    "        return self.annotaions.iloc[index, 0].strip()\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        filename = self.annotaions.iloc[index, 0].strip()\n",
    "        path = os.path.join(self.audio_dir, filename + \".wav\")\n",
    "        return path\n",
    "\n",
    "    def _get_audio_sample_output(self, index):\n",
    "        return self.annotaions.iloc[index, 1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        output = self._get_audio_sample_output(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path, normalize=True)\n",
    "        speaker_id = self._get_speaker_id(index)\n",
    "        return (signal, sr, output, speaker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechRecognitionModel(nn.Module):\n",
    "    def __init__(self, input_dim=13, num_classes=77, cnn_output_dim=200, gru_hidden_dim=256):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        \n",
    "        # CNN Layer\n",
    "        self.conv1 = nn.Conv1d(input_dim, cnn_output_dim, kernel_size=11, stride=1, padding=5)\n",
    "        self.bn1 = nn.BatchNorm1d(cnn_output_dim)\n",
    "        \n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(cnn_output_dim, gru_hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.bn2 = nn.BatchNorm1d(gru_hidden_dim * 2)\n",
    "        \n",
    "       # Dense Layer\n",
    "        self.fc = nn.Linear(gru_hidden_dim * 2, num_classes)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(gru_hidden_dim * 2, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, seq_length, cnn_output_dim)\n",
    "        x, _ = self.gru(x)\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, gru_hidden_dim * 2, seq_length)\n",
    "        x = self.bn2(x)\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, seq_length, gru_hidden_dim * 2)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def avg_wer(wer_scores, combined_ref_len):\n",
    "    return float(sum(wer_scores)) / float(combined_ref_len)\n",
    "\n",
    "\n",
    "def _levenshtein_distance(ref, hyp):\n",
    "    \"\"\"Levenshtein distance is a string metric for measuring the difference\n",
    "    between two sequences. Informally, the levenshtein disctance is defined as\n",
    "    the minimum number of single-character edits (substitutions, insertions or\n",
    "    deletions) required to change one word into the other. We can naturally\n",
    "    extend the edits to word level when calculate levenshtein disctance for\n",
    "    two sentences.\n",
    "    \"\"\"\n",
    "    m = len(ref)\n",
    "    n = len(hyp)\n",
    "\n",
    "    # special case\n",
    "    if ref == hyp:\n",
    "        return 0\n",
    "    if m == 0:\n",
    "        return n\n",
    "    if n == 0:\n",
    "        return m\n",
    "\n",
    "    if m < n:\n",
    "        ref, hyp = hyp, ref\n",
    "        m, n = n, m\n",
    "\n",
    "    # use O(min(m, n)) space\n",
    "    distance = np.zeros((2, n + 1), dtype=np.int32)\n",
    "\n",
    "    # initialize distance matrix\n",
    "    for j in range(0, n + 1):\n",
    "        distance[0][j] = j\n",
    "\n",
    "    # calculate levenshtein distance\n",
    "    for i in range(1, m + 1):\n",
    "        prev_row_idx = (i - 1) % 2\n",
    "        cur_row_idx = i % 2\n",
    "        distance[cur_row_idx][0] = i\n",
    "        for j in range(1, n + 1):\n",
    "            if ref[i - 1] == hyp[j - 1]:\n",
    "                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n",
    "            else:\n",
    "                s_num = distance[prev_row_idx][j - 1] + 1\n",
    "                i_num = distance[cur_row_idx][j - 1] + 1\n",
    "                d_num = distance[prev_row_idx][j] + 1\n",
    "                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n",
    "\n",
    "    return distance[m % 2][n]\n",
    "\n",
    "\n",
    "def word_errors(reference, hypothesis, ignore_case=False, delimiter=\" \"):\n",
    "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
    "    hypothesis sequence in word-level.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param delimiter: Delimiter of input sentences.\n",
    "    :type delimiter: char\n",
    "    :return: Levenshtein distance and word number of reference sentence.\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    if ignore_case == True:\n",
    "        reference = reference.lower()\n",
    "        hypothesis = hypothesis.lower()\n",
    "\n",
    "    ref_words = reference.split(delimiter)\n",
    "    hyp_words = hypothesis.split(delimiter)\n",
    "\n",
    "    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n",
    "    return float(edit_distance), len(ref_words)\n",
    "\n",
    "\n",
    "def char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n",
    "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
    "    hypothesis sequence in char-level.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param remove_space: Whether remove internal space characters\n",
    "    :type remove_space: bool\n",
    "    :return: Levenshtein distance and length of reference sentence.\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    if ignore_case == True:\n",
    "        reference = reference.lower()\n",
    "        hypothesis = hypothesis.lower()\n",
    "\n",
    "    join_char = \" \"\n",
    "    if remove_space == True:\n",
    "        join_char = \"\"\n",
    "\n",
    "    reference = join_char.join(filter(None, reference.split(\" \")))\n",
    "    hypothesis = join_char.join(filter(None, hypothesis.split(\" \")))\n",
    "\n",
    "    edit_distance = _levenshtein_distance(reference, hypothesis)\n",
    "    return float(edit_distance), len(reference)\n",
    "\n",
    "\n",
    "def wer(reference, hypothesis, ignore_case=False, delimiter=\" \"):\n",
    "    \"\"\"Calculate word error rate (WER). WER compares reference text and\n",
    "    hypothesis text in word-level. WER is defined as:\n",
    "    .. math::\n",
    "        WER = (Sw + Dw + Iw) / Nw\n",
    "    where\n",
    "    .. code-block:: text\n",
    "        Sw is the number of words subsituted,\n",
    "        Dw is the number of words deleted,\n",
    "        Iw is the number of words inserted,\n",
    "        Nw is the number of words in the reference\n",
    "    We can use levenshtein distance to calculate WER. Please draw an attention\n",
    "    that empty items will be removed when splitting sentences by delimiter.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param delimiter: Delimiter of input sentences.\n",
    "    :type delimiter: char\n",
    "    :return: Word error rate.\n",
    "    :rtype: float\n",
    "    :raises ValueError: If word number of reference is zero.\n",
    "    \"\"\"\n",
    "    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case, delimiter)\n",
    "\n",
    "    if ref_len == 0:\n",
    "        raise ValueError(\"Reference's word number should be greater than 0.\")\n",
    "\n",
    "    wer = float(edit_distance) / ref_len\n",
    "    return wer\n",
    "\n",
    "\n",
    "def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n",
    "    \"\"\"Calculate charactor error rate (CER). CER compares reference text and\n",
    "    hypothesis text in char-level. CER is defined as:\n",
    "    .. math::\n",
    "        CER = (Sc + Dc + Ic) / Nc\n",
    "    where\n",
    "    .. code-block:: text\n",
    "        Sc is the number of characters substituted,\n",
    "        Dc is the number of characters deleted,\n",
    "        Ic is the number of characters inserted\n",
    "        Nc is the number of characters in the reference\n",
    "    We can use levenshtein distance to calculate CER. Chinese input should be\n",
    "    encoded to unicode. Please draw an attention that the leading and tailing\n",
    "    space characters will be truncated and multiple consecutive space\n",
    "    characters in a sentence will be replaced by one space character.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param remove_space: Whether remove internal space characters\n",
    "    :type remove_space: bool\n",
    "    :return: Character error rate.\n",
    "    :rtype: float\n",
    "    :raises ValueError: If the reference length is zero.\n",
    "    \"\"\"\n",
    "    edit_distance, ref_len = char_errors(\n",
    "        reference, hypothesis, ignore_case, remove_space\n",
    "    )\n",
    "\n",
    "    if ref_len == 0:\n",
    "        raise ValueError(\"Length of reference should be greater than 0.\")\n",
    "\n",
    "    cer = float(edit_distance) / ref_len\n",
    "    return cer\n",
    "\n",
    "\n",
    "class TextTransform:\n",
    "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        char_map_str = \"\"\"\n",
    "        ' 0\n",
    "        अ  1\n",
    "        आ 2 \n",
    "        इ  3 \n",
    "        ई  4 \n",
    "        उ  5\n",
    "        ऊ  6\n",
    "        ए  7 \n",
    "        ऐ  8 \n",
    "        ओ 9 \n",
    "        औ 10\n",
    "        क 11\n",
    "        ख 12 \n",
    "        ग 13 \n",
    "        घ 14\n",
    "        ङ 15 \n",
    "        च 16\n",
    "        छ 17\n",
    "        ज 18 \n",
    "        झ 19 \n",
    "        ञ 20\n",
    "        ट 21 \n",
    "        ठ 22 \n",
    "        ड 23 \n",
    "        ढ 24 \n",
    "        ण 25 \n",
    "        त 26 \n",
    "        थ 27\n",
    "        द 28\n",
    "        ध 29\n",
    "        न 30 \n",
    "        प 31 \n",
    "        फ 32 \n",
    "        ब 33 \n",
    "        भ 34 \n",
    "        म 35\n",
    "        य 36\n",
    "        र 37\n",
    "        ल 38\n",
    "        व 39\n",
    "        श 40\n",
    "        ष 41\n",
    "        स 42\n",
    "        ह 43\n",
    "         ँ  44\n",
    "         ं  45\n",
    "         ः  46\n",
    "         ्  47 \n",
    "         ा  48\n",
    "         ि  49 \n",
    "         ी  50 \n",
    "         ु  51\n",
    "         ू  52\n",
    "         ृ  53\n",
    "         े  54\n",
    "         ै  55\n",
    "         ो  56\n",
    "         ौ  57\n",
    "        ॐ 58\n",
    "        ऋ 59\n",
    "        ।  60 \n",
    "        ०  61\n",
    "        १  62\n",
    "        २  63\n",
    "        ३  64\n",
    "        ४  65\n",
    "        ५  66\n",
    "        ६  67\n",
    "        ७  68\n",
    "        ८  69\n",
    "        ९  70\n",
    "        <SPACE> 71\n",
    "        \\u200c 72\n",
    "        \\u200d 73\n",
    "        . 74\n",
    "         ़  75\n",
    "        <UNK> 76\n",
    "        \"\"\"\n",
    "\n",
    "        self.char_map = {}\n",
    "        self.index_map = {}\n",
    "        for line in char_map_str.strip().split(\"\\n\"):\n",
    "            ch, index = line.split()\n",
    "            self.char_map[ch] = int(index)\n",
    "            self.index_map[int(index)] = ch\n",
    "        self.index_map[71] = \" \"\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        \"\"\"Use a character map and convert text to an integer sequence\"\"\"\n",
    "        int_sequence = []\n",
    "        for c in text:\n",
    "            if c == \" \":\n",
    "                ch = self.char_map[\"<SPACE>\"]\n",
    "            else:\n",
    "                ch = self.char_map.get(c, self.char_map[\"<UNK>\"])\n",
    "            int_sequence.append(ch)\n",
    "        return int_sequence\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "        \"\"\"Use a character map and convert integer labels to an text sequence\"\"\"\n",
    "        string = []\n",
    "        for i in labels:\n",
    "            string.append(self.index_map[i])\n",
    "        return \"\".join(string).replace(\"<SPACE>\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "NO_MFCC = 13\n",
    "NO_MELS = 26\n",
    "NO_FFT = 256\n",
    "WINDOW_LEN = int(0.025 * SAMPLE_RATE)  # 25ms window size\n",
    "HOP_LEN = int(0.01 * SAMPLE_RATE)  # 10ms hop size (commonly used)\n",
    "AUDIO_DIR = \"/Users/santoshpandey/Desktop/ASR/code/data/OpenSLR2/wavs\"\n",
    "ANNOTATIONS_FILE_PATH = \"/Users/santoshpandey/Desktop/ASR/code/data/OpenSLR2/line_index.tsv\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "mfcc_spectrogram = torchaudio.transforms.MFCC(sample_rate=SAMPLE_RATE,\n",
    "                                              n_mfcc=NO_MFCC,\n",
    "                                              melkwargs={\"n_fft\": NO_FFT,\n",
    "                                                         \"hop_length\": HOP_LEN,\n",
    "                                                         \"n_mels\": NO_MELS,                                                         \n",
    "                                                         \"center\": False})\n",
    "\n",
    "def compute_mfcc(audio_filename, mfcc_transform):\n",
    "    t_signal, t_sr = torchaudio.load(audio_filename, normalize=True)\n",
    "    return mfcc_transform(t_signal)\n",
    "\n",
    "def get_audio_filenames(audio_dir):\n",
    "    return [os.path.join(audio_dir, _filename) for _filename in os.listdir(audio_dir)]\n",
    "\n",
    "def calculate_average_mfccs(audio_dir, mfcc_transform):\n",
    "    \"\"\"\n",
    "    Calculate mean and standard deviation from a sample of the dataset, \n",
    "    which helps in normalization later.\n",
    "    \"\"\"\n",
    "    audio_filenames = get_audio_filenames(audio_dir)\n",
    "    mfccs = [compute_mfcc(_filename, mfcc_transform) for _filename in audio_filenames]\n",
    "    mfccs_stacked = torch.cat(mfccs, dim=2)\n",
    "    mean = mfccs_stacked.mean(dim=2, keepdim=True)\n",
    "    std = mfccs_stacked.std(dim=2, keepdim=True)\n",
    "    return (mean, std)\n",
    "\n",
    "mean, std = calculate_average_mfccs(AUDIO_DIR, mfcc_spectrogram)\n",
    "# print(f\"Mean: {mean}\\n std: {std}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_mfcc(mfcc, mean, std, epsilon=1e-8):\n",
    "    return (mfcc - mean) / (std + epsilon)\n",
    "\n",
    "def train_audio_transforms(waveform, mfcc_params):\n",
    "    mean = mfcc_params[\"mean\"]\n",
    "    std = mfcc_params[\"std\"]\n",
    "    spectrogram = mfcc_params[\"transform\"](waveform)\n",
    "    return normalize_mfcc(spectrogram, mean, std)\n",
    "\n",
    "def data_processing(data, text_transform, mfcc_params, data_type=\"train\"):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    \n",
    "    for waveform, _, utterance, _ in data:\n",
    "        if data_type == \"train\":\n",
    "            spec = train_audio_transforms(waveform, mfcc_params).squeeze(0).transpose(0, 1)\n",
    "        elif data_type == \"valid\":\n",
    "            spec = train_audio_transforms(waveform, mfcc_params).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            raise Exception(\"data_type should be train or valid\")\n",
    "        \n",
    "        spectrograms.append(spec)\n",
    "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
    "        labels.append(label)\n",
    "        \n",
    "        input_lengths.append(spec.shape[0] // 2)\n",
    "        label_lengths.append(len(label))\n",
    "    \n",
    "    spectrograms = (\n",
    "        nn.utils.rnn.pad_sequence(spectrograms, batch_first=True)\n",
    "        .unsqueeze(1)\n",
    "        .transpose(2, 3)\n",
    "    ).squeeze(1)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterMeter(object):\n",
    "    \"\"\"keeps track of total iterations\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.val += 1\n",
    "\n",
    "    def get(self):\n",
    "        return self.val\n",
    "\n",
    "def GreedyDecoder(\n",
    "    output, labels, label_lengths, text_transform, blank_label=77, collapse_repeated=True,\n",
    "):\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "    targets = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "        targets.append(\n",
    "            text_transform.int_to_text(labels[i][: label_lengths[i]].tolist())\n",
    "        )\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank_label:\n",
    "                if collapse_repeated and j != 0 and index == args[j - 1]:\n",
    "                    continue\n",
    "                decode.append(index.item())\n",
    "        decodes.append(text_transform.int_to_text(decode))\n",
    "    return decodes, targets\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    device,\n",
    "    train_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    epoch,\n",
    "    iter_meter,\n",
    "):\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "    \n",
    "    for batch_idx, _data in enumerate(train_loader):\n",
    "        spectrograms, labels, input_lengths, label_lengths = _data\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(spectrograms)  # (batch, time, n_class)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        output = output.transpose(0, 1)  # (time, batch, n_class)\n",
    "\n",
    "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        iter_meter.step()\n",
    "        if batch_idx % 10 == 0 or batch_idx == data_len:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(spectrograms),\n",
    "                    data_len,\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "def test(model, device, test_loader, criterion, epoch, iter_meter, text_transform):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_cer, test_wer = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, _data in enumerate(test_loader):\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1)  # (time, batch, n_class)\n",
    "\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "            decoded_preds, decoded_targets = GreedyDecoder(\n",
    "                output.transpose(0, 1), labels, label_lengths, text_transform\n",
    "            )\n",
    "\n",
    "            print(f\"Decoded targets: {decoded_targets}\")\n",
    "            print(f\"Decoded predicts: {decoded_preds}\\n\")\n",
    "\n",
    "            for j in range(len(decoded_preds)):\n",
    "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "\n",
    "    avg_cer = sum(test_cer) / len(test_cer)\n",
    "    avg_wer = sum(test_wer) / len(test_wer)\n",
    "\n",
    "    print(\n",
    "        \"Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n\".format(\n",
    "            test_loss, avg_cer, avg_wer\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dataset, text_transform, device, mfcc_params):\n",
    "    learning_rate = 0.015\n",
    "    batch_size = 50\n",
    "    epochs = 10\n",
    "    input_dim = 13  # Number of MFCC features\n",
    "    num_classes = len(text_transform.char_map)+1\n",
    "    \n",
    "    # Total dataset length\n",
    "    dataset_length = len(dataset)\n",
    "    \n",
    "    # Define the split lengths\n",
    "    train_length = int(0.8 * dataset_length)\n",
    "    test_length = dataset_length - train_length\n",
    "\n",
    "    # Split the dataset\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_length, test_length])\n",
    "\n",
    "    print(f\"Training dataset length: {len(train_dataset)}\")\n",
    "    print(f\"Testing dataset length: {len(test_dataset)}\")\n",
    "    \n",
    "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if device==\"cuda\" else {}\n",
    "    \n",
    "    train_loader = data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda x: data_processing(x, text_transform, mfcc_params, \"train\"),\n",
    "        **kwargs,\n",
    "    )\n",
    "    print(f\"Train Loader: {train_loader}\")\n",
    "\n",
    "    test_loader = data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: data_processing(x, text_transform, mfcc_params, \"valid\"),\n",
    "        **kwargs,\n",
    "    )\n",
    "    print(f\"Test Loader {test_loader}\")\n",
    "    \n",
    "    # Model, Optimizer, Loss\n",
    "    model = SpeechRecognitionModel(input_dim=input_dim, num_classes=num_classes).to(device)\n",
    "    print(f\"Model: {model}\")\n",
    "    print(\"Num Model Parameters\", sum([param.nelement() for param in model.parameters()]))\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-6, momentum=0.9)\n",
    "    ctc_loss = nn.CTCLoss(blank=77)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=learning_rate,\n",
    "        steps_per_epoch=int(len(train_loader)),\n",
    "        epochs=epochs,\n",
    "        anneal_strategy=\"linear\",\n",
    "    )\n",
    "\n",
    "    iter_meter = IterMeter()\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        train(model,\n",
    "            device,\n",
    "            train_loader,\n",
    "            ctc_loss,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            epoch,\n",
    "            iter_meter)\n",
    "        test(model, device, test_loader, ctc_loss, epoch, iter_meter, text_transform)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset length: 1650\n",
      "Testing dataset length: 413\n",
      "Train Loader: <torch.utils.data.dataloader.DataLoader object at 0x149c32510>\n",
      "Test Loader <torch.utils.data.dataloader.DataLoader object at 0x149c9b2f0>\n",
      "Model: SpeechRecognitionModel(\n",
      "  (conv1): Conv1d(13, 200, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (bn1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (gru): GRU(200, 256, batch_first=True, bidirectional=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=512, out_features=78, bias=True)\n",
      ")\n",
      "Num Model Parameters 773726\n",
      "Train Epoch: 1 [0/1650 (0%)]\tLoss: 40.265060\n"
     ]
    }
   ],
   "source": [
    "dataset = NepaliSoundDataset(\n",
    "    ANNOTATIONS_FILE_PATH, AUDIO_DIR\n",
    ")\n",
    "\n",
    "mfcc_params = {\n",
    "    \"transform\": mfcc_spectrogram.to(device),\n",
    "    \"sample_rate\": SAMPLE_RATE,\n",
    "    \"mean\": mean,\n",
    "    \"std\": std,\n",
    "}\n",
    "\n",
    "# print(f\"There are {len(dataset)} items\")\n",
    "# instance_row = dataset[890]\n",
    "# print(f\"Instance : {instance_row}\")\n",
    "# print(f\"Shape: {instance_row[0].shape}\")\n",
    "\n",
    "text_transform = TextTransform()\n",
    "main(dataset, text_transform, device, mfcc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC shape: torch.Size([1, 13, 1204])\n",
      "Mfcc: tensor([[[-2.1362e+02, -2.1397e+02, -2.1397e+02,  ..., -2.0130e+02,\n",
      "          -2.0128e+02, -2.0116e+02],\n",
      "         [ 4.9686e-01, -4.9400e-07, -4.9400e-07,  ...,  1.6210e+01,\n",
      "           1.7660e+01,  1.6479e+01],\n",
      "         [ 4.9337e-01, -5.8453e-06, -5.8453e-06,  ...,  1.6603e+01,\n",
      "           1.6818e+01,  1.7065e+01],\n",
      "         ...,\n",
      "         [ 3.8633e-01,  1.0954e-05,  1.0954e-05,  ...,  2.0957e+00,\n",
      "           2.1011e+00,  2.6708e+00],\n",
      "         [ 3.6396e-01, -1.1933e-05, -1.1933e-05,  ...,  2.2543e-01,\n",
      "           9.4376e-01,  5.8319e-01],\n",
      "         [ 3.3991e-01, -1.1608e-05, -1.1608e-05,  ...,  6.5242e-01,\n",
      "           1.3968e-01,  3.8388e-01]]])\n"
     ]
    }
   ],
   "source": [
    "audio_file = \"/Users/santoshpandey/Desktop/ASR/code/data/OpenSLR2/wavs/nep_0258_0119737288.wav\"\n",
    "\n",
    "waveform, sample_rate = torchaudio.load(audio_file, normalize=True)\n",
    "transform = torchaudio.transforms.MFCC(sample_rate=sample_rate,\n",
    "                                       n_mfcc=13,\n",
    "                                       melkwargs={\"n_fft\": 400, \"hop_length\": 160, \"n_mels\": 23, \"center\": False})\n",
    "mfcc = transform(waveform)\n",
    "print(f\"MFCC shape: {mfcc.shape}\")\n",
    "print(f\"Mfcc: {mfcc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spectrogram shape: torch.Size([1, 64, 377])\n",
      "spectrogram: tensor([[[2.7597e-04, 2.6752e-05, 7.5394e-06,  ..., 5.8833e-02,\n",
      "          1.9726e-01, 3.5975e-01],\n",
      "         [1.6730e-04, 1.9157e-05, 9.5878e-06,  ..., 3.0364e-03,\n",
      "          1.4986e-02, 3.3401e-02],\n",
      "         [1.1796e-05, 6.1857e-06, 7.6325e-06,  ..., 4.9778e-04,\n",
      "          4.8290e-04, 4.0218e-04],\n",
      "         ...,\n",
      "         [2.2947e-07, 1.8024e-07, 1.7291e-07,  ..., 1.0671e-04,\n",
      "          8.7168e-05, 1.0945e-04],\n",
      "         [2.4681e-07, 1.9094e-07, 1.3441e-07,  ..., 8.3326e-05,\n",
      "          5.5303e-05, 7.1069e-05],\n",
      "         [2.1231e-07, 1.2688e-07, 7.5778e-08,  ..., 8.9468e-05,\n",
      "          4.9146e-05, 4.6855e-05]]])\n"
     ]
    }
   ],
   "source": [
    "audio_file = \"/Users/santoshpandey/Desktop/ASR/code/data/OpenSLR2/wavs/nep_0258_0119737288.wav\"\n",
    "waveform, sample_rate = torchaudio.load(audio_file, normalize=True)\n",
    "transform = torchaudio.transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=1024, hop_length=512, n_mels=64)\n",
    "spectrogram = transform(waveform)\n",
    "print(f\"spectrogram shape: {spectrogram.shape}\")\n",
    "print(f\"spectrogram: {spectrogram}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
